# Microsoft Fabric â€“ End-to-End Data Engineering Project

## Project Overview

This project demonstrates a full end-to-end data engineering pipeline built on Microsoft Fabric, following real-world enterprise data architecture patterns.

The solution covers the complete lifecycle:

- Raw data ingestion  
- Data transformation  
- Incremental data processing (CDC-style)  
- Analytical data modeling  
- Warehouse loading for reporting and Power BI consumption  

The architecture follows the **Bronze â†’ Silver â†’ Gold** design pattern.

---

## Architecture Overview

**Source â†’ Lakehouse â†’ Warehouse â†’ Power BI**

### Data Flow

#### Bronze Layer
- Raw data ingestion from CSV (simulating an operational source system)
- Stored as Delta tables in Fabric Lakehouse

#### Silver Layer
- Data cleansing and type enforcement
- Timestamp standardization
- Business-ready schema preparation

#### Gold Layer
- Incremental MERGE (Upsert) logic using Delta Lake
- Idempotent design
- Business-consumable fact table

#### Warehouse Layer
- Gold data loaded into Fabric Warehouse
- Optimized for reporting and analytics
- Power BIâ€“ready tables

---

## Key Concepts Demonstrated

- Delta Lake & ACID transactions
- Incremental processing with MERGE INTO
- Idempotent data pipelines
- Lakehouse â†’ Warehouse integration
- Enterprise-ready data modeling
- Separation of concerns across layers

---

## Repository Structure

```text
fabric-end-to-end-data-engineering/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ bronze_ingestion.ipynb
â”‚   â”œâ”€â”€ silver_transform.ipynb
â”‚   â”œâ”€â”€ gold_merge.ipynb
â”‚   â””â”€â”€ warehouse_reporting.ipynb
â”‚
â”œâ”€â”€ pipelines/
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ warehouse/
â”‚   â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ powerbi/
â”‚   â”œâ”€â”€ kpi_total_revenue.png
â”‚   â”œâ”€â”€ kpi_total_quantity.png
â”‚   â”œâ”€â”€ revenue_over_time.png
â”‚   â”œâ”€â”€ revenue_by_product.png
â”‚   â””â”€â”€ sales_detail_table.png
â”‚
â”œâ”€â”€ diagrams/
â”‚   â””â”€â”€ .gitkeep
â”‚
â””â”€â”€ README.md

## Technologies Used

- Microsoft Fabric
- Lakehouse (Delta Tables)
- Fabric Warehouse
- PySpark
- Delta Lake
- Power BI (Direct Lake)

---

## Reporting & Analytics

The final Gold data is loaded into a Fabric Warehouse and is designed to be consumed directly by Power BI using Direct Lake for high-performance analytics and reporting.

---

## Incremental Data Processing

This project implements CDC-style incremental updates using:
- Business keys (SaleID)
- Delta Lake MERGE INTO
- Update & Insert logic to prevent duplicate records
The pipeline is safe to rerun multiple times without data duplication.

---

## Why This Project Matters

This project reflects how modern data engineering teams:
- Build scalable Lakehouse architectures
- Maintain clean separation between raw, refined, and analytical data
- Serve business intelligence teams with reliable, trusted datasets
It is designed to closely resemble real-world enterprise data engineering workflows.

---

## Future Enhancements

- Fabric Pipelines & Scheduling
- Automated incremental loads
- Power BI dashboards
- Data quality checks
- Monitoring & logging

---

## Visual Analytics (Fabric)

Visual analytics are demonstrated using Fabric-native notebook visualizations and Power BIâ€“style charts. These visuals
represent how the Gold layer data is ultimately consumed by analytics and BI teams via Fabric Warehouse and Direct Lake.

---

## ðŸ‘¤ Author

Alex Elamin
Data Engineer | SQL | Microsoft Fabric | Power BI

ðŸ”— GitHub: https://github.com/alex-elamin

ðŸ”— LinkedIn: https://www.linkedin.com/in/alexelamin
